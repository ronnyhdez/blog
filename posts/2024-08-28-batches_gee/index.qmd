---
title: "Exporting asset when exceding GEE API limit: batches"
description: ""
date: 2024-09-07
categories: [GEE, batch, API]
licence: "CC BY-NC"
image: lakes.png
author: Ronny A. Hernandez Mora
execute:
  message: false
  warning: false
format: 
  html:
    theme:
      - flatly
    linkcolor: "#FF5500"
    highlight-style: tango
    toc: true
    toc-title: Table of contents
    toc-location: left
    number-sections: false
    colorlinks: true
    code-fold: true
    code-line-numbers: true
editor: visual
jupyter: python3
editor_options: 
  chunk_output_type: console
---

So, we have geodatabase that we want to work with in Google Earth Engine. Turns out that we are working with the python API and after reading the necessary layer, we want to export it as an asset to GEE but we get the error: `ee.ee_exception.EEException: Request payload size exceeds the limit: 10485760 bytes.`

I don't want to export locally as a shape file, and then upload the file manually to GEE. I want this process to be in python code. How can we achieve this? Well, we will have to divide our data in batches and upload the batches. Nonetheless, it can be tricky because we will have a lot of assets, that we need as just one asset.

In this tutorial, I will show you the steps to make it possible.

# Read the data

I will use some polygons reservoirs from the province of Alberta, Canada. This dataset comes from ABMI ... TODO: complete the reference.

```{python}
import geopandas as gpd
import os
import sys
import janitor
import ee
import json
import math

import os
path = str(os.getcwd()) + '/data_check/HFI2021.gdb'
print(path)
reservoirs = gpd.read_file(path,
                           layer='o01_Reservoirs_HFI_2021')
```

Now, I don't need all the variables and also I want to clean the names. Also, I want to know the total number of rows and the CRS:

```{python}
reservoirs = reservoirs.clean_names()
reservoirs = reservoirs[['feature_ty', 'geometry']]

print(reservoirs.head())
print(f'Total number of reservoirs: {len(reservoirs)}')
print(reservoirs.crs)
```

Everything seems fine. At this point, if I try to export the data as it is, it will fail and return the API exceed limit message:

```{python}
# Initialize the GEE
ee.Initialize()

# Transform the dataframe to a json object
reservoirs_geojson = reservoirs.to_json()

# Load GeoJSON as an Earth Engine FeatureCollection
reservoirs_fc = ee.FeatureCollection(json.loads(reservoirs_geojson))
exportTask = ee.batch.Export.table.toAsset(
    collection=reservoirs_fc,
    description='Reservoirs Export',
    assetId='projects/ee-ronnyale/assets/reservoirs'
)

# Start the export task (This works for 10 reservoirs)
exportTask.start()
```

# Export in batches

So, we are exceeding the API rate limit. Instead of uploading the data all at once, I will create batches and upload them one by one.

This has one consequence: it will create many assets. But, we will fix this in the following steps.

For this, I will need to define the batch size, the number of batches and loop through the batches to upload them with a unique ID. For the unique ID I can assign the number of batch.

One important thing: when I read the data, it contains the original CRS. When uploading as it is, it will keep this CRS which it will create a distorsion on the polygons when working in GEE. To avoid this I'm going to change the CRS to `epsg=4326` which is the one used by GEE.

```{python}
ee.Initialize()

# Define the batch size
batch_size = 500

# Calculate the number of batches needed
num_batches = math.ceil(len(reservoirs) / batch_size)

for i, batch in enumerate(reservoirs.groupby(reservoirs.index // batch_size)):
    # Reproject to WGS 84 (EPSG:4326)
    batch = batch[1].to_crs(epsg=4326)

    # Convert to GeoJSON
    batch_geojson = batch.to_json()

    # Load GeoJSON as an Earth Engine FeatureCollection
    batch_fc = ee.FeatureCollection(json.loads(batch_geojson))

    # Define a unique asset ID for each batch
    batch_asset_id = f'projects/ee-ronnyale/assets/reservoirs_batch_{i+1}'

    # Export the batch to GEE
    print(f'Exporting the batch: {batch_asset_id}')
    exportTask = ee.batch.Export.table.toAsset(
        collection=batch_fc,
        description=f'Reservoirs Batch {i+1}',
        assetId=batch_asset_id
    )

    # Start the export task
    exportTask.start()
```

There we have. In total, there are 17 new assets in GEE. Now I need to merge all of them into one single asset.

TODO: include screenshot of the assets created

# Merge assets in one

I have to define in a list all the assets I want to merge. So, I will create a list taking advantage of the name pattern that the individual reservoirs assets have (just the final id batch number changes)

Then I will loop on each one and merge them. This will create an object that lives in GEE but it's not saved. That's why I will need to export it as the new merged asset

```{python}
batch_asset_ids = [f'projects/ee-ronnyale/assets/reservoirs_batch_{i+1}' for i in range(num_batches)]
reservoirs_fc = ee.FeatureCollection(batch_asset_ids[0])
for asset_id in batch_asset_ids[1:]:
    batch_fc = ee.FeatureCollection(asset_id)
    reservoirs_fc = reservoirs_fc.merge(batch_fc)

exportTask = ee.batch.Export.table.toAsset(
    collection=reservoirs_fc,
    description='Merged Reservoirs',
    assetId='projects/ee-ronnyale/assets/reservoirs_merged'
)
exportTask.start()
```

# Removing individual assets

Finally, we ended up with many reservoirs assets (one per each batch) and the merged reservoirs asset. I just need the final one, so I want to remove the rest. Instead of doing this manually click-ing each one in GEE, I'm can do it in a loop:

```{python}
for asset_id in batch_asset_ids:
    try:
        ee.data.deleteAsset(asset_id)
    except Exception as e:
```

# Steps summary

Here is the complete steps with some `prints()` that will inform the user what is happening when running the code:

```{python}
ee.Initialize()

# Define the batch size
batch_size = 500

# Calculate the number of batches needed
num_batches = math.ceil(len(reservoirs) / batch_size)
print(f'Total of batches to export: {num_batches}')

# for i, batch in enumerate(reservoirs.groupby(reservoirs.index // batch_size)):
#     # Reproject to WGS 84 (EPSG:4326)
#     batch = batch[1].to_crs(epsg=4326)

#     # Convert to GeoJSON
#     batch_geojson = batch.to_json()

#     # Load GeoJSON as an Earth Engine FeatureCollection
#     print(f'Transformin to json batch {i}')
#     batch_fc = ee.FeatureCollection(json.loads(batch_geojson))

#     # Define a unique asset ID for each batch
#     batch_asset_id = f'projects/ee-ronnyale/assets/reservoirs_batch_{i+1}'

#     # Export the batch to GEE
#     print(f'Exporting the batch: {batch_asset_id}')
#     exportTask = ee.batch.Export.table.toAsset(
#         collection=batch_fc,
#         description=f'Reservoirs Batch {i+1}',
#         assetId=batch_asset_id
#     )

#     # Start the export task
#     exportTask.start()


# Steps to join batches in 1 collection:
## I have the number of batches to create the list
## 

# batch_asset_ids = [f'projects/ee-ronnyale/assets/reservoirs_batch_{i+1}' for i in range(num_batches)]
# reservoirs_fc = ee.FeatureCollection(batch_asset_ids[0])
# for asset_id in batch_asset_ids[1:]:
#     batch_fc = ee.FeatureCollection(asset_id)
#     reservoirs_fc = reservoirs_fc.merge(batch_fc)

# print(f'Total number of features: {reservoirs_fc.size().getInfo()}')

# # Example: Export the merged collection to another asset (if needed)
# exportTask = ee.batch.Export.table.toAsset(
#     collection=reservoirs_fc,
#     description='Merged Reservoirs',
#     assetId='projects/ee-ronnyale/assets/reservoirs_merged'
# )
# exportTask.start()

# for asset_id in batch_asset_ids:
#     try:
#         ee.data.deleteAsset(asset_id)
#         print(f'Successfully deleted: {asset_id}')
#     except Exception as e:
#         print(f'Failed to delete {asset_id}: {e}')
```
