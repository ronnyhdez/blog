---
title: "A Step-by-Step Guide to Exporting 'Large' Datasets to Google Earth Engine via Python"
description: "If you've encountered the dreaded _Request payload size exceeds the limit_ error, you're not alone."
date: 2024-09-07
categories: [GEE, batch, API]
licence: "CC BY-NC"
image: alberta.png
jupyter: python3
editor_options: 
  chunk_output_type: console
bibliography: ref.bib
---

Have you ever found yourself pre-processing a local geodatabase, expecting to
use the cleaned version on Google Earth Engine (GEE), only to get stuck when
you try to export it as an asset? Most likely, you exceed the [quota limit on GEE](https://developers.google.com/earth-engine/guides/usage) and saw this message: `ee.ee_exception.EEException: Request payload size exceeds the limit: 10485760 bytes.`

This limitation can be frustrating, especially when you want to maintain a Python-based workflow without resorting to manual uploads (i.e. saving your
cleaned data to a `.shp` file and uploading it through the GEE Asset Manager
in the Code Editor). In this tutorial, we'll explore a programmatic solution to overcome GEE's upload limit.

We'll work on a approach that allows us to:

 1. Divide our data into manageable chunks.
 2. Export these batches as assets to GEE programmatically.
 3. Consolidate the uploaded batches into a single asset within GEE.

## Setting Up the Environment

Before we dive into the main process, let's set up our environment and import 
the necessary libraries and initialize GEE:

```{python}
import geopandas as gpd
import os
import sys
import janitor
import ee
import json
import math

# Initialize Google Earth Engine
ee.Initialize()
```

# Read and pre-process the data

For this tutorial, we'll use a dataset of reservoirs polygons from the province
of Alberta, Canada. This dataset is provided by the Alberta Biodiversity 
Monitoring Institute (ABMI) [@abmi_hfi_2021].

```{python}
# Set the path to your geodatabase
path = str(os.getcwd()) + '/data_check/HFI2021.gdb'
# print(f'Reading data from: {path}')

# Read the specific layer from the geodatabase
reservoirs = gpd.read_file(path, layer = 'o01_Reservoirs_HFI_2021')

# Clean column names and select necessary columns
reservoirs = reservoirs.clean_names()
reservoirs = reservoirs[['feature_ty', 'geometry']]

print(reservoirs.head())
print(f'Total number of reservoirs: {len(reservoirs)}')
print(f'CRS: {reservoirs.crs}')
```

In this step, we read a specific layer from our dataset, clean the column names,
and select only the necessary columns. While additional pre-processing steps can
be performed locally, in this case, we focus on selecting the specific data we 
need. Additionally, we print the number of observations included in the layer
and the CRS.

# The API Limit Challenge

If we try to export this entire dataset to GEE at once, we'll encounter an error
due to the API's payload size limit:

::: {.column-margin}
From this point forward, whenever you see 'projects/ee-ronnyale/assets/', be 
sure to replace it with your own GEE project path.
:::
```{python}
#| error: true

# This code will raise an error due to payload size
reservoirs_geojson = reservoirs.to_json()
reservoirs_fc = ee.FeatureCollection(json.loads(reservoirs_geojson))
exportTask = ee.batch.Export.table.toAsset(
    collection = reservoirs_fc,
    description = 'Cleaned Reservoirs Export',
    assetId = 'projects/ee-ronnyale/assets/reservoirs'
)
exportTask.start()
```

# Exporting Data in Batches

To overcome the API rate limit, we'll divide our data into manageable batches 
and upload them individually. This approach has a trade-off: it will create
multiple assets in GEE, which we'll need to merge later. Here's how we'll do it:

 1. Define a batch size
 2. Calculate the number of batches needed
 3. Loop through the data, creating and uploading each batch
 4. Assign a unique ID to each batch for easy identification

One crucial step: before uploading, we'll change the CRS to `epsg=4326`, which 
is used by GEE. This prevents distortion of the polygons when working in GEE.

```{python}
#| error: true

# Define the batch size
batch_size = 500

# Calculate the number of batches needed
num_batches = math.ceil(len(reservoirs) / batch_size)

for i, batch in enumerate(reservoirs.groupby(reservoirs.index // batch_size)):
    # Reproject to WGS 84 (EPSG:4326)
    batch = batch[1].to_crs(epsg = 4326)

    # Convert to GeoJSON
    batch_geojson = batch.to_json()

    # Load GeoJSON as an Earth Engine FeatureCollection
    batch_fc = ee.FeatureCollection(json.loads(batch_geojson))

    # Define a unique asset ID for each batch
    batch_asset_id = f'projects/ee-ronnyale/assets/reservoirs_batch_{i+1}'

    # Export the batch to GEE
    print(f'Exporting the batch: {batch_asset_id}')
    exportTask = ee.batch.Export.table.toAsset(
        collection = batch_fc,
        description = f'Reservoirs Batch {i+1}',
        assetId = batch_asset_id
    )

    # Start the export task
    exportTask.start()
```

There we have. In total, there are 17 new assets in GEE. Now I need to merge 
all of them into one single asset.

On your GEE assets you should be able to see something like:

![Batches as assets in GEE](images/reservoirs_batches.png)

## Merging Assets in GEE

Now that we have all our data in GEE as separate assets, we need to merge them
into a single dataset. First, we'll create a list of all the batches already
created. We'll loop through the total number of batches to get the unique batch
IDs in a list. Afterward, iterate through each batch and merge them into a 
single object. This object will exist in GEE but it's not saved. That's why 
we'll need to export it as the new merged asset. 

```{python}
#| error: true
# Create a list of all batch asset IDs
batch_asset_ids = [f'projects/ee-ronnyale/assets/reservoirs_batch_{i+1}' for i in range(num_batches)]

# Merge all batches into a single FeatureCollection
reservoirs_fc = ee.FeatureCollection(batch_asset_ids[0])
for asset_id in batch_asset_ids[1:]:
    batch_fc = ee.FeatureCollection(asset_id)
    reservoirs_fc = reservoirs_fc.merge(batch_fc)

print(f'Total number of features in merged collection: {reservoirs_fc.size().getInfo()}')

# Export the merged collection as a new asset
exportTask = ee.batch.Export.table.toAsset(
    collection = reservoirs_fc,
    description = 'Merged Reservoirs',
    assetId = 'projects/ee-ronnyale/assets/reservoirs_merged'
)
exportTask.start()
print("Merged asset export task started. Check your GEE Tasks tab for progress.")
```

This step creates a new single asset that contains all our data. We can use it
later in our analysis as a single FeatureCollection.

# Cleaning Up: Removing Individual Assets

Finally, let's clean up by removing the individual batch assets, keeping only 
our merged dataset:

```{python}
#| error: true
for asset_id in batch_asset_ids:
    try:
        ee.data.deleteAsset(asset_id)
        print(f'Successfully deleted: {asset_id}')
    except Exception as e:
        print(f'Failed to delete {asset_id}: {e}')

print("Cleanup complete. Check your GEE Assets to confirm.")
```

