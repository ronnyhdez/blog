{
  "hash": "e9e28044eedc97256a02ff6d17bd8c70",
  "result": {
    "markdown": "---\ntitle: A Step-by-Step Guide to Exporting 'Large' Datasets to Google Earth Engine via Python\ndescription: 'If you''ve encountered the dreaded _Request payload size exceeds the limit_ error, you''re not alone.'\ndate: '2024-09-09'\ncategories:\n  - GEE\n  - batch\n  - API\nlicence: CC BY-NC\nimage: alberta.png\neditor_options:\n  chunk_output_type: console\nbibliography: ref.bib\n---\n\nHave you ever found yourself pre-processing a local geodatabase, expecting to\nuse the cleaned version on Google Earth Engine (GEE), only to get stuck when\nyou try to export it as an asset? Most likely, you exceed the [quota limit on GEE](https://developers.google.com/earth-engine/guides/usage) and saw this message: `ee.ee_exception.EEException: Request payload size exceeds the limit: 10485760 bytes.`\n\nThis limitation can be frustrating, especially when you want to maintain a Python-based workflow without resorting to manual uploads (i.e. saving your\ncleaned data to a `.shp` file and uploading it through the GEE Asset Manager\nin the Code Editor). In this tutorial, we'll explore a programmatic solution to overcome GEE's upload limit.\n\nWe'll work on a approach that allows us to:\n\n 1. Divide our data into manageable chunks.\n 2. Export these batches as assets to GEE programmatically.\n 3. Consolidate the uploaded batches into a single asset within GEE.\n\n## Setting Up the Environment\n\nBefore we dive into the main process, let's set up our environment and import \nthe necessary libraries and initialize GEE:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport geopandas as gpd\nimport os\nimport sys\nimport janitor\nimport ee\nimport json\nimport math\n\n# Initialize Google Earth Engine\nee.Initialize()\n```\n:::\n\n\n# Read and pre-process the Data\n\nFor this tutorial, we'll use a dataset of reservoirs polygons from the province\nof Alberta, Canada. This dataset is provided by the Alberta Biodiversity \nMonitoring Institute (ABMI) [@abmi_hfi_2021].\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Set the path to your geodatabase\npath = str(os.getcwd()) + '/data_check/HFI2021.gdb'\n# print(f'Reading data from: {path}')\n\n# Read the specific layer from the geodatabase\nreservoirs = gpd.read_file(path, layer = 'o01_Reservoirs_HFI_2021')\n\n# Clean column names and select necessary columns\nreservoirs = reservoirs.clean_names()\nreservoirs = reservoirs[['feature_ty', 'geometry']]\n\nprint(reservoirs.head())\nprint(f'Total number of reservoirs: {len(reservoirs)}')\nprint(f'CRS: {reservoirs.crs}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  feature_ty                                           geometry\n0  RESERVOIR  MULTIPOLYGON (((807480.895 5943839.731, 807492...\n1  RESERVOIR  MULTIPOLYGON (((721133.55 6291704.327, 721119....\n2  RESERVOIR  MULTIPOLYGON (((716030.577 6294731.491, 715977...\n3  RESERVOIR  MULTIPOLYGON (((817815.182 5949939.667, 817857...\n4  RESERVOIR  MULTIPOLYGON (((388703.975 6217051.101, 388699...\nTotal number of reservoirs: 8101\nCRS: EPSG:3400\n```\n:::\n:::\n\n\nIn this step, we read a specific layer from our dataset, cleaned the column\nnames, and selected only the necessary columns. While additional pre-processing\nsteps can be performed locally, in this case, we focus on selecting the specific\ndata we need. Additionally, we print the number of observations included in the layer and the CRS.\n\n# The API Limit Challenge\n\nIf we try to export this entire dataset as it is to GEE at once, we'll \nencounter the mentioned error due to the API's payload size limit:\n\n::: {.column-margin}\nFrom this point forward, whenever you see 'projects/ee-ronnyale/assets/', be \nsure to replace it with your own GEE project path.\n:::\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# This code will raise an error due to payload size\nreservoirs_geojson = reservoirs.to_json()\nreservoirs_fc = ee.FeatureCollection(json.loads(reservoirs_geojson))\nexportTask = ee.batch.Export.table.toAsset(\n    collection = reservoirs_fc,\n    description = 'Cleaned Reservoirs Export',\n    assetId = 'projects/ee-ronnyale/assets/reservoirs'\n)\nexportTask.start()\n```\n\n::: {.cell-output .cell-output-error}\n```\nEEException: Request payload size exceeds the limit: 10485760 bytes.\n```\n:::\n:::\n\n\n# Exporting Data in Batches\n\nTo overcome the API rate limit, we'll divide our data into manageable batches \nand upload them individually. This approach has a trade-off: it will create\nmultiple assets in GEE, which we'll need to merge later. Here's how we'll do it:\n\n::: {.column-margin}\n*How to know the batch size we should use? In my case it was about trial and\nerror. I knew from the code above that I had 8101 observations, so batches of\n500 seemed fine given that each observation contains one round-ish polygon. This\ncould change if we have much more complicated polygons, which will be bigger in\nsize (bytes not area) and hence, that extra information will force us to \ndownsize the number of observations per each batch. \n:::\n\n 1. Define a batch size*\n 2. Calculate the number of batches needed\n 3. Loop through the data, creating and uploading each batch\n 4. Assign a unique ID to each batch for easy identification\n\nOne crucial step: before uploading, we'll change the CRS to `epsg=4326`, which \n[is used by GEE](https://developers.google.com/earth-engine/guides/table_upload).\nThis prevents distortion of the polygons when working in GEE.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Define the batch size\nbatch_size = 500\n\n# Calculate the number of batches needed\nnum_batches = math.ceil(len(reservoirs) / batch_size)\n\nfor i, batch in enumerate(reservoirs.groupby(reservoirs.index // batch_size)):\n    # Reproject to WGS 84 (EPSG:4326)\n    batch = batch[1].to_crs(epsg = 4326)\n\n    # Convert to GeoJSON\n    batch_geojson = batch.to_json()\n\n    # Load GeoJSON as an Earth Engine FeatureCollection\n    batch_fc = ee.FeatureCollection(json.loads(batch_geojson))\n\n    # Define a unique asset ID for each batch\n    batch_asset_id = f'projects/ee-ronnyale/assets/reservoirs_batch_{i+1}'\n\n    # Export the batch to GEE\n    print(f'Exporting the batch: {batch_asset_id}')\n    exportTask = ee.batch.Export.table.toAsset(\n        collection = batch_fc,\n        description = f'Reservoirs Batch {i+1}',\n        assetId = batch_asset_id\n    )\n\n    # Start the export task\n    exportTask.start()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nExporting the batch: projects/ee-ronnyale/assets/reservoirs_batch_1\nExporting the batch: projects/ee-ronnyale/assets/reservoirs_batch_2\nExporting the batch: projects/ee-ronnyale/assets/reservoirs_batch_3\nExporting the batch: projects/ee-ronnyale/assets/reservoirs_batch_4\nExporting the batch: projects/ee-ronnyale/assets/reservoirs_batch_5\nExporting the batch: projects/ee-ronnyale/assets/reservoirs_batch_6\nExporting the batch: projects/ee-ronnyale/assets/reservoirs_batch_7\nExporting the batch: projects/ee-ronnyale/assets/reservoirs_batch_8\nExporting the batch: projects/ee-ronnyale/assets/reservoirs_batch_9\nExporting the batch: projects/ee-ronnyale/assets/reservoirs_batch_10\nExporting the batch: projects/ee-ronnyale/assets/reservoirs_batch_11\nExporting the batch: projects/ee-ronnyale/assets/reservoirs_batch_12\nExporting the batch: projects/ee-ronnyale/assets/reservoirs_batch_13\nExporting the batch: projects/ee-ronnyale/assets/reservoirs_batch_14\nExporting the batch: projects/ee-ronnyale/assets/reservoirs_batch_15\nExporting the batch: projects/ee-ronnyale/assets/reservoirs_batch_16\nExporting the batch: projects/ee-ronnyale/assets/reservoirs_batch_17\n```\n:::\n:::\n\n\nOn your GEE assets you should be able to see something like:\n\n![Batches as assets in GEE](images/reservoirs_batches.png)\n\n## Merging Assets in GEE\n\nNow that we have all our data in GEE as separate assets, we need to merge them\ninto a single dataset. First, we'll create a list of all the batches already\ncreated. We'll loop through the total number of batches to get the unique batch\nIDs in a list. Afterward, iterate through each batch and merge them into a \nsingle object. This object will exist in GEE but it's not saved. That's why \nwe'll need to export it as the new merged asset. \n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Create a list of all batch asset IDs\nbatch_asset_ids = [f'projects/ee-ronnyale/assets/reservoirs_batch_{i+1}' for i in range(num_batches)]\n\n# Merge all batches into a single FeatureCollection\nreservoirs_fc = ee.FeatureCollection(batch_asset_ids[0])\nfor asset_id in batch_asset_ids[1:]:\n    batch_fc = ee.FeatureCollection(asset_id)\n    reservoirs_fc = reservoirs_fc.merge(batch_fc)\n\nprint(f'Total number of features in merged collection: {reservoirs_fc.size().getInfo()}')\n\n# Export the merged collection as a new asset\nexportTask = ee.batch.Export.table.toAsset(\n    collection = reservoirs_fc,\n    description = 'Merged Reservoirs',\n    assetId = 'projects/ee-ronnyale/assets/reservoirs_merged'\n)\nexportTask.start()\nprint(\"Merged asset export task started. Check your GEE Tasks tab for progress.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal number of features in merged collection: 8101\nMerged asset export task started. Check your GEE Tasks tab for progress.\n```\n:::\n:::\n\n\n:::{.column-margin}\nI like to print the final number of observations, so I can be sure that all the\noriginal observations are there. This could be translated into an automated test\nfor the project.\n:::\n\nThis step creates a new single asset that contains all our data. We can use it\nlater in our analysis as a single FeatureCollection.\n\n# Cleaning Up: Removing Individual Assets\n\nFinally, let's clean up by removing the individual batch assets, keeping only \nour merged dataset:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfor asset_id in batch_asset_ids:\n    try:\n        ee.data.deleteAsset(asset_id)\n        print(f'Successfully deleted: {asset_id}')\n    except Exception as e:\n        print(f'Failed to delete {asset_id}: {e}')\n\nprint(\"Cleanup complete. Check your GEE Assets to confirm.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSuccessfully deleted: projects/ee-ronnyale/assets/reservoirs_batch_1\nSuccessfully deleted: projects/ee-ronnyale/assets/reservoirs_batch_2\nSuccessfully deleted: projects/ee-ronnyale/assets/reservoirs_batch_3\nSuccessfully deleted: projects/ee-ronnyale/assets/reservoirs_batch_4\nSuccessfully deleted: projects/ee-ronnyale/assets/reservoirs_batch_5\nSuccessfully deleted: projects/ee-ronnyale/assets/reservoirs_batch_6\nSuccessfully deleted: projects/ee-ronnyale/assets/reservoirs_batch_7\nSuccessfully deleted: projects/ee-ronnyale/assets/reservoirs_batch_8\nSuccessfully deleted: projects/ee-ronnyale/assets/reservoirs_batch_9\nSuccessfully deleted: projects/ee-ronnyale/assets/reservoirs_batch_10\nSuccessfully deleted: projects/ee-ronnyale/assets/reservoirs_batch_11\nSuccessfully deleted: projects/ee-ronnyale/assets/reservoirs_batch_12\nSuccessfully deleted: projects/ee-ronnyale/assets/reservoirs_batch_13\nSuccessfully deleted: projects/ee-ronnyale/assets/reservoirs_batch_14\nSuccessfully deleted: projects/ee-ronnyale/assets/reservoirs_batch_15\nSuccessfully deleted: projects/ee-ronnyale/assets/reservoirs_batch_16\nSuccessfully deleted: projects/ee-ronnyale/assets/reservoirs_batch_17\nCleanup complete. Check your GEE Assets to confirm.\n```\n:::\n:::\n\n\nAnd we are done! We were able to export from our local computer\nthe selected layer and all it's observations as an asset to GEE. Be aware that\nbigger datasets can take a lot of time uploading! \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}